# 机器学习

## 统计学与监督学习概论

统计学习三要素

+ 模型
+ 评价标准——模型参数选择的准则
+ 算法——优化模型的算法

## 感知机

### 模型

$f(x)=sign(\omega \cdot x+b)$

### 评价标准

对于误分类数据$(x_i,y_i)$来说

$-y_i(\omega \cdot x+b)>0$

因此，误分类点到超平面的距离是

$- \frac{1}{||\omega||}y_i(\omega \cdot x_i+b)$

不考虑$\frac{1}{||\omega||}$得到损失函数

$L(\omega,b)=-\sum_{x_i\in M}y_i(\omega \cdot x_i+b)$

转化为求参数$\omega,b$的损失函数极小化问题

$\min\limits_{\omega,b}L(\omega,b)=-\sum\limits_{x_i\in M}y_i(\omega \cdot x_i+b)$

### 算法

梯度下降法，一次==随机选取==一个误分类点使其梯度下降

随时函数梯度由

$\nabla_\omega L(\omega ,b)=-\sum\limits_{x_i\in M}y_ix_i$

$\nabla_b L(\omega ,b)=-\sum\limits_{x_i\in M}y_i$

给出。选出一个误分类点，对$\omega,b$进行更新：

$w\leftarrow\omega +\eta y_ix_i$

$b\leftarrow b+\eta y_i$

## K近邻

### 算法

1. 根据给定的==距离度量==，在训练集T中找出与x最近邻的k个点，==涵盖这k个点的x邻域==记作$N_k(x)$

2. 在$N_k(x)$中根据分类决策（如多数表决）决定x的类别y

   $y=arg\max\limits_{c_j}\sum\limits_{x_i\in N_k(x)}I(y_i=c_i)~,~i=1,2,…,K$

### 评价标准

多数表决（等价于经验风险最小化），加权表决等

### KD树

二叉树

对K维空间的划分

## 朴素贝叶斯

属于生成模型

分类的特征条件独立

### 模型

$y=f(x)=arg\max\limits_{c_k}\frac{P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum\limits_kP(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}$

由于对于任意的$c_k$分母是相同的，所以：

$y=arg\max\limits_{c_k}P(Y=c_k)\prod\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)$

### 评价标准

后验概率最大化准则（根据期望风险最小化原则得到）

期望风险函数

$R_{exp}(f)=E[L(Y,f(X))]$

$minR_{exp}(f)=E_X\sum\limits^K_{k=1}[L(c_k,f(X))]P(c_k|X)$

对$X=x$逐个极小化

$f(x)=arg\min\limits_{y\in \gamma}\sum\limits^{K}_{k=1}L(c_k,y)P(c_k|X=x)$

$=arg\min\limits_{y\in \gamma}\sum\limits^{K}_{k=1}P(y\ne c_k|X=x)$

$=arg\min\limits_{y\in \gamma}\sum\limits^{K}_{k=1}(1-P(y=c_k|X=x))$

$=arg\max\limits_{y\in \gamma}\sum\limits^{K}_{k=1}P(y=c_k|X=x)$

即后验概率最大化准则

### 算法

1. 先计算先验概率和条件概率

   先验概率：$P(Y=c_k)=\frac{\sum\limits^N_{i=1}I(y_i=c_k)}{N}~,~k=1,2,…,K$

   条件概率：$P(x^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits^N_{i=1}I(x^{(j)}=a_{jl},y_i=c_k)}{\sum\limits^N_{i=1}I(y_i=c_k)}$

   $j=1,2,…,n$表示x的维度

   $l=1,2,…,S_j$表示第j维度的特征类别

   $k=1,2,…,K$表示分类结果空间

2. 对给定的实例$X=(x^{(1)},x^{(2)},…,x^{(n)})$

   计算$P(Y=c_k)\prod\limits^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k)$

3. 找出最大项对应的$c_k$

   即：$y=arg\max \limits_{c_k} P(Y=c_k)\prod\limits^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k)$

## 决策树

推理过程容易理解，If-Then结构

完全依赖==属性变量==的取值特点

可以忽略==贡献较低==的属性变量

信息增益：特征A对数据集D的信息增益：$g(D,A)=H(D)-H(D|A)$

信息增益比：$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$

​	其中，$H_A(D)$是数据D关于特征A的熵$H_A(D)=-\sum\limits^n_{i=1}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$

### ID3与C4.5算法

#### 模型

If-Then树状（可以是多叉树）结构对样本空间进行划分，由==内部节点==和==叶结点==组成

内部节点表示==一个特征或属性==

叶子结点表示==一个类==

#### 评价标准（特征选择）

ID3：选择==信息增益==最大的特征

C4.5：选择==信息增益比==最大的特征

#### 算法

1. 对于每个特征$A_a$计算

   $g(D,A_i)=H(D)-H(D|A_a)$

   $H(D)=-\sum\limits^K_{k=1}\frac{|c_k|}{|D|}log_2\frac{|c_k|}{|D|}$

   特征$A$将样本划分为n个子集$D_1,D_2,…,D_n$,n为A的不同取值的个数

   记子集$D_i$中属于$c_k$类的样本为$D_{ik}=c_k\cap D_i$

   得到$H(D|A)=-\sum\limits^n_{i=1}\frac{|D_i|}{|D|}\sum\limits^K_{k=1}\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}$

   C4.5算法计算$g_R(D,A_i)=\frac{g(D,A_i)}{H_A(D)}$

2. 选择==信息增益==或==信息增益比==最大的特征$A_g$，依据$A_g$的每个可能值$a_i$将D划分为n个区域，每个子集构成一个子节点，选择==实例数最大==的类作为子集的标记
3. 对每个结点i，以$D_i$为训练集，以$A-\{A_g\}$为特征集重复1~2步

### Cart树

#### 模型

==二叉==决策树模型

回归树表示为：$f(x)=\sum\limits^M_{m=1}c_mI(x\in R_m)$

每个$R_m$单元上有固定输出$c_m$

#### 评价标准（特征选择）

==基尼系数==最小化准则

基尼系数：$Gini(p)=\sum\limits^K_{k=1}p_k(1-p_k)=1-\sum\limits^K_{k=1}p_k^2$

*二分类问题：$Gini(p)=2p(1-p)$*

如果样本D被特征A分为$D_1$和$D_2$(==二叉决策树==)

$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$

$Gini(D_i)$是在==$D_i$子集中==根据==标签$c_k$==计算

#### 算法

1. 对每个特征A以及它的每个a值，根据A是否等于a将D分成$D_1$和$D_2$并计算$Gini(D,A=a)$
2. 先在特征A的取值a中找到==基尼系数最小==的切分点，再找到所有==特征的切分点中基尼系数最小==的==特征==，将数据集按照找到的==特征==和==切分点==进行划分，分别放到两个子节点中
3. 递归调用1、2步骤

## 逻辑斯地回归与最大熵

### Logistic 分布模型

分布函数：$F(x)=P(X\leqslant x)=\frac{1}{1+e^{-(x-\mu)/\gamma}}$

密度函数：$f(x)=F’(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2}$

$\mu$为==位置==参数，$\gamma>0$为==形状==参数 

分布函数关于$(\mu,\frac{1}{2})$中心对称

$F(-x+\mu)-\frac{1}{2}=\frac{1}{2}-F(x+\mu)$

### Logistic回归

几率odds：$\frac{p}{1-p}$，发生比上不发生

对数几率：$logit(p)=log\frac{p}{1-p}$

二项Logistic回归：

$P(Y=1|x)=\frac{exp(\omega\cdot x)}{1+exp(\omega\cdot x)}$

$P(Y=0|x)=\frac{1}{1+exp(\omega\cdot x)}$

此时将权值向量和输入向量进行了==扩充==

$\omega=(\omega^{(1)},\omega^{(2)},…,\omega^{(n)},b)^T$

$x=(x^{(1)},x^{(2)},…,x^{(n)},1)^T$

对数Logistic回归：$log\frac{P(Y-1|x)}{1-P(Y=1|x)}=\omega\cdot x$

### 参数估计(评价标准)

Logistic分类器是是权值系数组成的，关键问题就是获得这组权值$\omega=(\omega^{(1)},\omega^{(2)},…,\omega^{(n)},b)$

采用==极大似然估计==来获得参数$\omega$

+ 似然函数：给定输入$x$时，关于参数$\theta$的似然函数等于给定参数$\theta$后变量X的概率$ L(\theta|x)=P(X=x|\theta)$
+ 极大似然函数：==使得似然函数取得最大值==的参数能够使得统计模型最为合理

### 算法

对于N个观测事件，设

$P(Y=1|x)=\pi(x)$

$P(Y=0|x)=1-\pi(x)$

其联合概率密度函数，即似然函数为：

$\prod\limits^N_{i=1}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$

对似然函数取对数，得到对数似然函数：

$L(\omega)=\sum\limits^N_{i=1}[y_ilog\pi(x_i)+(1-y_i)log(1-\pi(x_i)]$

$=\sum\limits^N_{i=1}[y_ilog\frac{\pi(x_i)}{1-\pi(x_i)}+log(1-\pi(x_i))]$

$=\sum\limits^N_{i=1}[y_i(\omega \cdot x)=log(1+exp(\omega\cdot x))]$

令$\frac{\partial L(\omega)}{\partial \omega}=0$求$L(\omega)$的极大值，得到$\omega$的估计$\overset{\frown}{\omega}$

将参数代入得出学习后的模型

### 多项Logistic

$P(Y=k|x)=\frac{exp(\omega_k\cdot x)}{1+\sum\limits^{K-1}_{k=1}exp(\omega_k\cdot x)}~,~k=1,2,…,K-1$

$P(Y=K|x)=\frac{1}{1+\sum\limits^{K-1}_{k=1}exp(\omega_k\cdot x)}$



## 支持向量机

### 模型

决策函数：$f(x)=sign(\omega^*\cdot x+b)$

### 评价标准

间隔最大化

当样本$(x_i,y_i)$被正确划分时，点$x_i$到超平面距离为：

$\gamma_i=y_i(\frac{\omega}{||\omega||}\cdot x_i+\frac{b}{||\omega||})$（几何间隔）

将问题转化为==约束优化问题==

$\max\limits_{\omega,b}~\gamma$

$s.t.~y_i(\frac{\omega}{||\omega||}\cdot x_i+\frac{b}{||\omega||})\ge \gamma,~i=1,2,…,N $

可以将问题改写成包含函数间隔的形式

$\max\limits_{\omega,b}~\frac{\widehat\gamma}{||\omega||}$

$s.t.~y_i(\omega\cdot x+b)\ge\widehat\gamma,~i=1,2,…,N$

==由于$\gamma$的大小并不影响优化问题的结果==，不妨取1

且最大化$\frac{1}{||\omega||}$与==最小化$\frac{1}{2}||\omega||^2$是等价==的

因此问题转化为

$\min\limits_{\omega,b}~\frac{1}{2}||\omega||^2$

$s.t.~y_i(\omega\cdot x+b)\ge1,~i=1,2,…,N$

为凸规划问题

==还可以解释为最小化合页损失函数==

$min~\sum\limits^N_{i=2}[1-y_i(\omega\cdot x_i+b)]_++\lambda||\omega||$

最后一项为==正则项==

### 算法

拉格朗日对偶

将原始问题转化为

$\min\limits_\alpha~\frac{1}{2}\sum\limits^N_{i=1}\sum\limits^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum\limits^N_{i=1}\alpha_i$

$s.t.~\sum\limits^N_{i=1}\alpha_iy_i=0$

$\alpha_i\ge0~,~i=1,2,…,N$

求解最优解$\alpha^*=(\alpha^*_1,\alpha^*_2,…,\alpha^*_N)^T$

其中$\alpha^*_j\ge0$的分量所对应的向量为==支持向量==

### 软间隔问题

原始问题：

$\min\limits_{\omega,b}~\frac{1}{2}||\omega||^2+C\sum\limits^N_{i=1}\xi_i$

$s.t.~y_i(\omega\cdot x+b)\ge1,~i=1-\xi_i~,~i=1,2,…,N$

$C$为惩罚参数

对偶问题：

$\min\limits_\alpha~\frac{1}{2}\sum\limits^N_{i=1}\sum\limits^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum\limits^N_{i=1}\alpha_i$

$s.t.~\sum\limits^N_{i=1}\alpha_iy_i=0$

$0\le\alpha_i\le C~,~i=1,2,…,N$

**==支持向量有四类==**

1. 在间隔边界上：$\alpha^*_i<C,\xi_i=0$
2. 在间隔边界和分离超平面之间：$\alpha^*_i=C,0<\xi_i<1$
3. 在分离超平面上：$\alpha^*_i=C,\xi_i=1$
4. 在分离超平面误分一侧：$\alpha^*_i=C,\xi_i>1$

### 非线性问题采用==核技巧==

将$x$映射到一个特征空间$\phi(x)$==转化为线性SVM==问题

用核函数表示映射后的内积，即：==$\phi(x_i)\cdot\phi(x_j)=K(x_i,x_j)$==

对偶问题转化为：

$\min\limits_\alpha~\frac{1}{2}\sum\limits^N_{i=1}\sum\limits^N_{j=1}\alpha_i\alpha_jy_iy_jK(x_i，x_j)-\sum\limits^N_{i=1}\alpha_i$

$s.t.~\sum\limits^N_{i=1}\alpha_iy_i=0$

$0\le\alpha_i\le C~,~i=1,2,…,N$

选择$\alpha^*=(\alpha^*_1,\alpha^*_2,…,\alpha^*_N)^T$

计算$b^*=y_j-\sum\limits^N_{i=1}\alpha^*_iy_iK(x_i,x_j)$

构造决策函数：

$f(x)=sign(\sum\limits^N_{i=1}\alpha^*_iy_iK(x,x_j)+b^*)$

当$K(x,z)$是正定核时，是凸规划问题，解存在

## 提升方法

通过改变==训练样本==的权重，学习多个学习器，并将学习器==进行线性组合==，提升分类性能

+ 每一轮提高==错分类样本==的权重$\omega_{mi}$
+ 加权多数表决，加大==分类误差小==的弱分类器的权重$\alpha_m$

### AdaBoost算法

1. 初始化训练集数据的权重

   $D_1=(\omega_{11},…,\omega_{1i},…,\omega_{1N}),~\omega_{1i}=\frac{1}{N},~i=1,2,…,N$

   N为训练集样本的个数

2. 对m个弱分类器$m=1,2,…,M$

   1. 使用具有权值分布$D_m$的数据集学习，得到分类器$G_m(x):X\rightarrow\{-1,+1\}$

   2. 计算$G_m$的==训练误差==：$e_m=P(G_m(x_i)\ne y_i)=\sum\limits^N_{i=1}\omega_i I(G_m(x_i)\ne y_i)$

   3. 计算分类器$G_m(x)$的系数：$\alpha_i=\frac{1}{2}log\frac{1-e_m}{e_m}$

   4. 由于分类器$e_m<50\%$，因此$\frac{1-e_m}{e_m}>1$，$\alpha_m>0$==($\alpha_m$未必小于1)==

   5. 更新训练集权值：$D_{m+1}=(\omega_{m+1,1},…,\omega_{m+1,i},…,\omega_{m+1,N})$

      其中$\omega_{m+1,i}=\frac{\omega_{m,1}}{Z_m}exp(-\alpha_my_iG_m(x_i))$

      $Z_m=\sum\limits^N_{i=1}\omega_{m+1,i}exp(-\alpha_iy_iG_m(x_i))$为规范化因子（保证$\sum\limits^N_{i=1}w_{m+1,i}=1$）

3. 得到最终分类器$G(x)=sign(f(x))=sign(\sum\limits^M_{m=1}\alpha_mG_m(x))$

### 提升树算法

#### 模型

加速模型（基函数加权的线性组合）

前向分布算法

决策树：==二叉分类树==或==二叉回归树==

$f_m(x)=\sum\limits^M_{m=1}T(x;\Theta_m)$

$\Theta_m$为第m棵决策树的参数

决策树将X划分为J个==互不相交的区域==$R_1,R_2,…,R_J$，每个区域对应输出常量$c_j$，树可以表示为：

$T(x;\Theta)=\sum\limits^J_{j=1}c_jI(x\in R_j)$

$\Theta=\{(R_1，c_1),(R_2，c_2),…,(R_J，c_J)\}$

#### 评价标准

==经验风险==极小化

$\widehat\Theta_m=arg\min\limits_{\Theta_m}\sum\limits^N_{i=1}L(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))$

采用平方损失时：

$L(y,f_{m-1}(x)+T（x;\Theta_m))=[y-f_{m-1}(x)-T(x;\Theta_m)]^2$

$=[r-T(x;\Theta_m)]^2$，$r=y-f_{m-1}(x)$记为==残差==

#### 算法

1. 初始化$f_0(x)=0$

2. 对$m=1,2,…,M$

   1. 计算残差$r_{mi}=y_i-f_{m-1}(x_i),~i=1,2,…,N$

   2. 拟合残差数据集$T_m=\{(x_1,r_{m1}),(x_1,r_{m2}),…,(x_1,r_{mN})\}$

      学习一个回归树$T_m(x;\Theta_m)$

   3. 更新$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$

3. 得到回归提升树：

   $f_M(x)=\sum\limits^M_{m=1}T(x;\Theta_m)$